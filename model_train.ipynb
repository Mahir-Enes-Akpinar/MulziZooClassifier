{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDXmymRCpBwf"
      },
      "source": [
        "# Yeni Bölüm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZi_6nYRBTcN",
        "outputId": "574b68f9-bdaa-403b-8ab4-f84bff321a3b"
      },
      "outputs": [],
      "source": [
        "!pip install rembg onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555,
          "referenced_widgets": [
            "cf49d92faa024eb79eb75281f3a507f4",
            "1e7b88db180f45edbe6b7ac875794637",
            "79c6fef1fa2c45ca87ff20a6a915d382",
            "aaf300d7bc9c438e926f5d1310aa9156",
            "395edb5a8bb044a193f97ec867f2476c",
            "5617cae0a4904bcda4fb296b09f4a07f",
            "8d79de9096364a06a00d362d5bb276e7",
            "b3b0bb6d58b64bfba35e0f61a285ffe2",
            "e4572d8ae57149a3b0752a91b9c9f9d2",
            "17f02d2bba5b4412ae64e43f772f8a0b",
            "ee66c63843714143a77e70e934f9615d"
          ]
        },
        "id": "5GQjmxtgv5e8",
        "outputId": "48996c80-5e75-41b0-d787-dbe2ba98beac"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from rembg import remove\n",
        "import io\n",
        "import cv2\n",
        "\n",
        "# === AŞAMA 1: ÖN İŞLEME ===\n",
        "def preprocess_multizoo(input_root, output_root_orig, output_root_edge):\n",
        "    classes = sorted(os.listdir(input_root))\n",
        "    for cls in tqdm(classes, desc=\"Sınıflar\"):\n",
        "        input_class_path = os.path.join(input_root, cls)\n",
        "        output_class_orig = os.path.join(output_root_orig, cls)\n",
        "        output_class_edge = os.path.join(output_root_edge, cls)\n",
        "        os.makedirs(output_class_orig, exist_ok=True)\n",
        "        os.makedirs(output_class_edge, exist_ok=True)\n",
        "\n",
        "        for file in os.listdir(input_class_path):\n",
        "            if not file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                continue\n",
        "\n",
        "            input_path = os.path.join(input_class_path, file)\n",
        "            try:\n",
        "                with open(input_path, \"rb\") as f:\n",
        "                    input_bytes = f.read()\n",
        "                output_bytes = remove(input_bytes)\n",
        "                img = Image.open(io.BytesIO(output_bytes)).convert('RGB')\n",
        "                img_np = np.array(img)\n",
        "\n",
        "                r, g, b = cv2.split(img_np)\n",
        "                r_eq = cv2.equalizeHist(r)\n",
        "                g_eq = cv2.equalizeHist(g)\n",
        "                b_eq = cv2.equalizeHist(b)\n",
        "                hist_eq = cv2.merge((r_eq, g_eq, b_eq))\n",
        "\n",
        "                denoised = cv2.medianBlur(hist_eq, 5)\n",
        "\n",
        "                processed_img = Image.fromarray(denoised)\n",
        "                processed_img.save(os.path.join(output_class_orig, file))\n",
        "\n",
        "                gray = cv2.cvtColor(denoised, cv2.COLOR_RGB2GRAY)\n",
        "                edges = cv2.Canny(gray, 100, 200)\n",
        "                edge_rgb = np.stack((edges,)*3, axis=-1)\n",
        "                edge_img = Image.fromarray(edge_rgb)\n",
        "                edge_img.save(os.path.join(output_class_edge, file))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"{input_path} işlenemedi: {e}\")\n",
        "\n",
        "# Örnek kullanım (bir defa çağırman yeterli)\n",
        "preprocess_multizoo('/content/drive/MyDrive/train', '/content/drive/MyDrive/processed/train', '/content/drive/MyDrive/processed/train_edges')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "_P49QNtoxlox",
        "outputId": "5b57a7e9-b8d0-4266-b9fe-4875186723e1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eYaqz-zM1x9x",
        "outputId": "46498096-1d24-414d-a3ed-3d53d2e10fe2"
      },
      "outputs": [],
      "source": [
        "# MultiZoo Veri Seti için Transformer Tabanlı Sınıflandırma Modeli\n",
        "# Colab'de çalıştırmak ve modeli kaydetmek için\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "# Google Drive'ı bağla\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Veri seti yolu\n",
        "DATA_DIR = '/content/drive/MyDrive/train'\n",
        "EDGE_DIR = '/content/drive/MyDrive/processed/train_edges'\n",
        "  # Veri seti dizinini kendi yolunuza göre değiştirin\n",
        "\n",
        "# Sonuç klasörü\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/MultiZoo/output'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Cihaz kontrolü\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. Veri Seti Sınıfı\n",
        "\n",
        "\n",
        "\n",
        "class MultiZooProcessedDataset(Dataset):\n",
        "    def __init__(self, orig_root, edge_root, transform=None, edge_transform=None):\n",
        "        self.orig_root = orig_root\n",
        "        self.edge_root = edge_root\n",
        "        self.transform = transform\n",
        "        self.edge_transform = edge_transform\n",
        "\n",
        "        self.classes = sorted(os.listdir(orig_root))\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        for cls in self.classes:\n",
        "            class_dir = os.path.join(orig_root, cls)\n",
        "            for file in os.listdir(class_dir):\n",
        "                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    self.image_paths.append((os.path.join(class_dir, file),\n",
        "                                             os.path.join(edge_root, cls, file)))\n",
        "                    self.labels.append(self.class_to_idx[cls])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        orig_path, edge_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            orig_img = Image.open(orig_path).convert('RGB')\n",
        "            edge_img = Image.open(edge_path).convert('RGB')\n",
        "\n",
        "            if self.transform:\n",
        "                orig_img = self.transform(orig_img)\n",
        "            if self.edge_transform:\n",
        "                edge_img = self.edge_transform(edge_img)\n",
        "\n",
        "            return orig_img, edge_img, label\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Hata: {orig_path} veya {edge_path} yüklenemedi: {e}\")\n",
        "            dummy = torch.zeros((3, 224, 224))\n",
        "            return dummy, dummy, label\n",
        "\n",
        "# 2. Veri Dönüşümleri\n",
        "# Eğitim verileri için daha agresif veri artırma\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.3),  # Bazı hayvan türleri için kullanışlı olabilir\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Doğrulama ve test için daha basit dönüşümler\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Kenar görüntüleri için dönüşümler\n",
        "edge_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 3. Model Tanımı - Çift Girişli ViT\n",
        "class DualInputViT(nn.Module):\n",
        "    def __init__(self, num_classes=90, pretrained=True):\n",
        "        super(DualInputViT, self).__init__()\n",
        "\n",
        "        # İki ayrı görüntü akışı için Vision Transformer\n",
        "        # Colab için daha hafif bir model (ViT Small) kullanıyoruz\n",
        "        self.vit_original = timm.create_model('vit_small_patch16_224', pretrained=pretrained, num_classes=0)\n",
        "        self.vit_edge = timm.create_model('vit_small_patch16_224', pretrained=pretrained, num_classes=0)\n",
        "\n",
        "        # Özellik boyutunu al - ViT Small: 384, ViT Base: 768\n",
        "        embed_dim = self.vit_original.embed_dim\n",
        "\n",
        "        # Öznitelikleri birleştirdikten sonraki sınıflandırma katmanları\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embed_dim * 2, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_original, x_edge):\n",
        "        # Her iki transformer'dan özellikleri çıkar\n",
        "        feat_original = self.vit_original(x_original)\n",
        "        feat_edge = self.vit_edge(x_edge)\n",
        "\n",
        "        # Özellikleri birleştir\n",
        "        combined_features = torch.cat((feat_original, feat_edge), dim=1)\n",
        "\n",
        "        # Sınıflandırma\n",
        "        output = self.classifier(combined_features)\n",
        "\n",
        "        return output\n",
        "\n",
        "# 4. Veri Yükleyicileri Oluştur\n",
        "\n",
        "\n",
        "def create_data_loaders(batch_size=16, val_split=0.2):\n",
        "    # Tek bir dataset'ten hem orijinal hem edge verileri alınır\n",
        "    full_dataset = MultiZooProcessedDataset(\n",
        "        orig_root=DATA_DIR,\n",
        "        edge_root=EDGE_DIR,\n",
        "        transform=train_transforms,\n",
        "        edge_transform=edge_transforms\n",
        "    )\n",
        "\n",
        "    # Split oranına göre boyutları belirle\n",
        "    val_size = int(len(full_dataset) * val_split)\n",
        "    train_size = len(full_dataset) - val_size\n",
        "\n",
        "    # Eğitim ve val setlerini böl\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    # DataLoader oluştur\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Dataset istatistiklerini yaz\n",
        "    print(f\"Eğitim görüntü sayısı: {len(train_dataset)}\")\n",
        "    print(f\"Doğrulama görüntü sayısı: {len(val_dataset)}\")\n",
        "    print(f\"Sınıf sayısı: {len(full_dataset.classes)}\")\n",
        "\n",
        "    return train_loader, val_loader, full_dataset\n",
        "\n",
        "# 5. Eğitim ve Değerlendirme Fonksiyonları\n",
        "def train_epoch(model, loader, criterion, optimizer, device, accumulation_steps=4):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i, (orig_imgs, edge_imgs, labels) in enumerate(tqdm(loader, desc=\"Training\")):\n",
        "        orig_imgs, edge_imgs, labels = orig_imgs.to(device), edge_imgs.to(device), labels.to(device)\n",
        "\n",
        "        # İleri geçiş\n",
        "        outputs = model(orig_imgs, edge_imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Gradient accumulation\n",
        "        loss = loss / accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        # Her 'accumulation_steps' adımda optimizer güncelleme\n",
        "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(loader):\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # İstatistikler\n",
        "        running_loss += loss.item() * accumulation_steps\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # F1 skoru için tahminleri topla\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Epoch sonuçları\n",
        "    train_loss = running_loss / len(loader)\n",
        "    train_acc = correct / total\n",
        "    train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return train_loss, train_acc, train_f1\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for orig_imgs, edge_imgs, labels in tqdm(loader, desc=\"Validating\"):\n",
        "            orig_imgs, edge_imgs, labels = orig_imgs.to(device), edge_imgs.to(device), labels.to(device)\n",
        "\n",
        "            # İleri geçiş\n",
        "            outputs = model(orig_imgs, edge_imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # İstatistikler\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            # F1 skoru için tahminleri topla\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Sonuçlar\n",
        "    val_loss = running_loss / len(loader)\n",
        "    val_acc = correct / total\n",
        "    val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    # Ek metrikler\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    return val_loss, val_acc, val_f1, precision, recall, all_preds, all_labels\n",
        "\n",
        "# 6. Öğrenme Eğrilerini Görselleştirme\n",
        "def plot_training_curves(history):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Doğruluk eğrisi\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(history['train_acc'], label='Train')\n",
        "    plt.plot(history['val_acc'], label='Validation')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Kayıp eğrisi\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(history['train_loss'], label='Train')\n",
        "    plt.plot(history['val_loss'], label='Validation')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # F1 skor eğrisi\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(history['train_f1'], label='Train')\n",
        "    plt.plot(history['val_f1'], label='Validation')\n",
        "    plt.title('F1 Score')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.legend()\n",
        "\n",
        "    # Öğrenme oranı eğrisi\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(history['lr'])\n",
        "    plt.title('Learning Rate')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.yscale('log')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'training_curves.png'))\n",
        "    plt.show()\n",
        "\n",
        "# 7. Ana Eğitim Döngüsü\n",
        "def train_model(model, train_loader, val_loader, num_epochs=30, patience=5, batch_size=16, lr=1e-4):\n",
        "    # Model, kayıp fonksiyonu ve optimize edici\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Kayıp fonksiyonu - Sınıf dengesizliği için ağırlıklandırma\n",
        "    class_counts = np.bincount(train_loader.dataset.dataset.labels)\n",
        "\n",
        "    weights = torch.FloatTensor(1.0 / (class_counts + 1e-10))\n",
        "    weights = weights / weights.sum() * len(class_counts)\n",
        "    weights = weights.to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "    # Optimizasyon\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "    # Öğrenme oranı çizelgesi\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "    # Erken durdurma için değişkenler\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [],\n",
        "               'train_f1': [], 'val_f1': [], 'lr': []}\n",
        "\n",
        "    # Gradient accumulation adımları\n",
        "    accumulation_steps = 4  # Her 4 mini-batch'den sonra güncelleme\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Eğitim\n",
        "        train_loss, train_acc, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device, accumulation_steps)\n",
        "\n",
        "        # Doğrulama\n",
        "        val_loss, val_acc, val_f1, precision, recall, _, _ = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Öğrenme oranını güncelle\n",
        "        scheduler.step(val_f1)  # F1 skoruna göre öğrenme oranını ayarla\n",
        "\n",
        "        # Geçerli öğrenme oranı\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Sonuçları kaydet\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['train_f1'].append(train_f1)\n",
        "        history['val_f1'].append(val_f1)\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        # Sonuçları yazdır\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, LR: {current_lr:.6f}\")\n",
        "\n",
        "        # En iyi modeli kaydet\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'val_f1': val_f1,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'class_to_idx': train_loader.dataset.dataset.class_to_idx,\n",
        "                'classes': train_loader.dataset.dataset.classes\n",
        "            }\n",
        "            torch.save(checkpoint, os.path.join(OUTPUT_DIR, 'multizoo_dual_vit_best.pth'))\n",
        "            print(f\"Model kaydedildi! Doğrulama Skoru: {val_acc:.4f}\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Erken durdurma\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Erken durdurma! {patience} epoch boyunca iyileşme olmadı.\")\n",
        "            break\n",
        "\n",
        "    # Son modeli kaydet (en iyi olmasa bile)\n",
        "    checkpoint = {\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_acc': val_acc,\n",
        "        'val_f1': val_f1,\n",
        "        'class_to_idx': train_loader.dataset.dataset.class_to_idx,\n",
        "        'classes': train_loader.dataset.dataset.classes\n",
        "    }\n",
        "    torch.save(checkpoint, os.path.join(OUTPUT_DIR, 'multizoo_dual_vit_final.pth'))\n",
        "\n",
        "    # Eğitim eğrilerini görselleştir\n",
        "    plot_training_curves(history)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# 8. Örnek Tahmin Fonksiyonu (Test Görüntüleri İçin)\n",
        "def predict_and_visualize(model, image_path, class_names, transform, edge_transform, device):\n",
        "    # Görüntüyü yükle\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # Kenar algılama\n",
        "    img_np = np.array(img)\n",
        "    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "    edges_rgb = np.stack((edges,) * 3, axis=-1)\n",
        "    edge_img = Image.fromarray(edges_rgb)\n",
        "\n",
        "    # Dönüşümleri uygula\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "    edge_tensor = edge_transform(edge_img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Tahmin yap\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_tensor, edge_tensor)\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        conf, pred_idx = torch.max(probs, 1)\n",
        "\n",
        "    # Tahmin edilen sınıf ve güven skoru\n",
        "    predicted_class = class_names[pred_idx.item()]\n",
        "    confidence = conf.item()\n",
        "\n",
        "    # Tüm sınıflar için olasılıkları al\n",
        "    probs = probs.cpu().numpy()[0]\n",
        "\n",
        "    # En yüksek 5 tahmini getir\n",
        "    top5_idx = np.argsort(probs)[-5:][::-1]\n",
        "    top5_classes = [class_names[i] for i in top5_idx]\n",
        "    top5_probs = probs[top5_idx]\n",
        "\n",
        "    # Görüntüyü, kenar görüntüsünü ve sonuçları göster\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Orijinal görüntü\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title('Orijinal Görüntü')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Kenar görüntüsü\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(edges_rgb)\n",
        "    plt.title('Kenar Görüntüsü')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Top-5 tahminler\n",
        "    plt.subplot(1, 3, 3)\n",
        "    y_pos = np.arange(len(top5_classes))\n",
        "    plt.barh(y_pos, top5_probs)\n",
        "    plt.yticks(y_pos, top5_classes)\n",
        "    plt.title('Top-5 Tahminler')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Tahmin Edilen Sınıf: {predicted_class}\")\n",
        "    print(f\"Güven Skoru: {confidence:.4f}\")\n",
        "\n",
        "    return predicted_class, confidence, top5_classes, top5_probs\n",
        "\n",
        "# 9. Ana Fonksiyon\n",
        "def main():\n",
        "    print(\"MultiZoo Veri Seti için Transformer Tabanlı Sınıflandırma Modeli Eğitimi\")\n",
        "\n",
        "    # Veri yükleyicileri\n",
        "    batch_size = 16  # Colab için batch size - GPU belleğine göre ayarlayın\n",
        "    train_loader, val_loader, dataset = create_data_loaders(batch_size)\n",
        "\n",
        "\n",
        "    # Model\n",
        "    num_classes = len(dataset.classes)\n",
        "\n",
        "    model = DualInputViT(num_classes=num_classes, pretrained=True)\n",
        "\n",
        "    # Eğitim parametreleri\n",
        "    num_epochs = 30\n",
        "    patience = 5\n",
        "    learning_rate = 1e-4\n",
        "\n",
        "    # Modeli eğit\n",
        "    trained_model, history = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        num_epochs=num_epochs,\n",
        "        patience=patience,\n",
        "        batch_size=batch_size,\n",
        "        lr=learning_rate\n",
        "    )\n",
        "\n",
        "    print(f\"Model eğitimi tamamlandı ve modeller {OUTPUT_DIR} dizinine kaydedildi.\")\n",
        "\n",
        "    # Model mimarisini ve öğrenme eğrilerini içeren bir özet dosyası oluştur\n",
        "    with open(os.path.join(OUTPUT_DIR, 'model_summary.txt'), 'w') as f:\n",
        "        f.write(\"MultiZoo Transformer Modeli Özeti\\n\")\n",
        "        f.write(\"================================\\n\\n\")\n",
        "        f.write(f\"Eğitim veri seti boyutu: {len(train_dataset)}\\n\")\n",
        "        f.write(f\"Doğrulama veri seti boyutu: {len(val_dataset)}\\n\")\n",
        "        f.write(f\"Sınıf sayısı: {num_classes}\\n\\n\")\n",
        "        f.write(\"Model Mimarisi:\\n\")\n",
        "        f.write(\"- İki Vision Transformer (ViT Small) girişli\\n\")\n",
        "        f.write(\"- Özellik birleştirme ve sınıflandırma katmanları\\n\\n\")\n",
        "        f.write(\"Eğitim Parametreleri:\\n\")\n",
        "        f.write(f\"- Batch size: {batch_size}\\n\")\n",
        "        f.write(f\"- Başlangıç öğrenme oranı: {learning_rate}\\n\")\n",
        "        f.write(f\"- Maksimum epoch sayısı: {num_epochs}\\n\")\n",
        "        f.write(f\"- Erken durdurma sabrı: {patience}\\n\\n\")\n",
        "        f.write(\"En İyi Model Metrikleri:\\n\")\n",
        "        f.write(f\"- Doğrulama doğruluğu: {max(history['val_acc']):.4f}\\n\")\n",
        "        f.write(f\"- Doğrulama F1 skoru: {max(history['val_f1']):.4f}\\n\")\n",
        "\n",
        "    print(\"Model özeti oluşturuldu ve kaydedildi.\")\n",
        "\n",
        "    # Test örneği - eğer test veri seti mevcutsa\n",
        "    test_dir = os.path.join(DATA_DIR, 'test')\n",
        "    if os.path.exists(test_dir):\n",
        "        print(\"\\nTest görüntüleri üzerinde örnek tahminler yapılıyor...\")\n",
        "\n",
        "        # Tüm sınıf klasörlerini dön\n",
        "        for class_name in sorted(os.listdir(test_dir))[:3]:  # İlk 3 sınıfı dene\n",
        "            class_dir = os.path.join(test_dir, class_name)\n",
        "            if os.path.isdir(class_dir):\n",
        "                # Her sınıftan bir görüntü al\n",
        "                img_files = [f for f in os.listdir(class_dir)\n",
        "                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "                if img_files:\n",
        "                    img_path = os.path.join(class_dir, img_files[0])\n",
        "                    print(f\"\\nGerçek sınıf: {class_name}\")\n",
        "                    predict_and_visualize(\n",
        "                        trained_model,\n",
        "                        img_path,\n",
        "                        train_dataset.classes,\n",
        "                        val_transforms,\n",
        "                        edge_transforms,\n",
        "                        device\n",
        "                    )\n",
        "    else:\n",
        "        print(\"\\nTest veri seti bulunamadı. Test görüntüleri daha sonra denenebilir.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eov7sZtJilAK",
        "outputId": "d8c663c5-6333-42b6-b367-0101bb541d56"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# Cihaz\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model sınıfı (önceden tanımlanmalı)\n",
        "class DualInputViT(nn.Module):\n",
        "    def __init__(self, num_classes=90, pretrained=True):\n",
        "        super(DualInputViT, self).__init__()\n",
        "        import timm\n",
        "        self.vit_original = timm.create_model('vit_small_patch16_224', pretrained=pretrained, num_classes=0)\n",
        "        self.vit_edge = timm.create_model('vit_small_patch16_224', pretrained=pretrained, num_classes=0)\n",
        "        embed_dim = self.vit_original.embed_dim\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embed_dim * 2, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_original, x_edge):\n",
        "        feat_original = self.vit_original(x_original)\n",
        "        feat_edge = self.vit_edge(x_edge)\n",
        "        combined = torch.cat((feat_original, feat_edge), dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "# Kayıtlı modeli yükleme\n",
        "checkpoint_path = '/content/drive/MyDrive/MultiZoo/output/multizoo_dual_vit_best.pth'\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "# Sınıf sayısı\n",
        "num_classes = len(checkpoint['classes'])\n",
        "\n",
        "# Modeli oluştur ve ağırlıkları yükle\n",
        "model = DualInputViT(num_classes=num_classes)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model başarıyla yüklendi!\")\n",
        "\n",
        "# === TXT dosyalarını yaz ===\n",
        "output_dir = '/content/drive/MyDrive/MultiZoo/output'\n",
        "\n",
        "# classes.txt\n",
        "with open(os.path.join(output_dir, 'classes.txt'), 'w', encoding='utf-8') as f:\n",
        "    for cls in checkpoint['classes']:\n",
        "        f.write(f\"{cls}\\n\")\n",
        "\n",
        "# class_to_idx.txt\n",
        "with open(os.path.join(output_dir, 'class_to_idx.txt'), 'w', encoding='utf-8') as f:\n",
        "    for cls, idx in checkpoint['class_to_idx'].items():\n",
        "        f.write(f\"{cls}:{idx}\\n\")\n",
        "\n",
        "print(\"Sınıf dosyaları oluşturuldu: classes.txt ve class_to_idx.txt\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17f02d2bba5b4412ae64e43f772f8a0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e7b88db180f45edbe6b7ac875794637": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5617cae0a4904bcda4fb296b09f4a07f",
            "placeholder": "​",
            "style": "IPY_MODEL_8d79de9096364a06a00d362d5bb276e7",
            "value": "Sınıflar: 100%"
          }
        },
        "395edb5a8bb044a193f97ec867f2476c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5617cae0a4904bcda4fb296b09f4a07f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79c6fef1fa2c45ca87ff20a6a915d382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3b0bb6d58b64bfba35e0f61a285ffe2",
            "max": 90,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4572d8ae57149a3b0752a91b9c9f9d2",
            "value": 90
          }
        },
        "8d79de9096364a06a00d362d5bb276e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aaf300d7bc9c438e926f5d1310aa9156": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17f02d2bba5b4412ae64e43f772f8a0b",
            "placeholder": "​",
            "style": "IPY_MODEL_ee66c63843714143a77e70e934f9615d",
            "value": " 90/90 [2:50:07&lt;00:00, 114.18s/it]"
          }
        },
        "b3b0bb6d58b64bfba35e0f61a285ffe2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf49d92faa024eb79eb75281f3a507f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e7b88db180f45edbe6b7ac875794637",
              "IPY_MODEL_79c6fef1fa2c45ca87ff20a6a915d382",
              "IPY_MODEL_aaf300d7bc9c438e926f5d1310aa9156"
            ],
            "layout": "IPY_MODEL_395edb5a8bb044a193f97ec867f2476c"
          }
        },
        "e4572d8ae57149a3b0752a91b9c9f9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee66c63843714143a77e70e934f9615d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
